{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw2_bir.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3zbFwd2DgW"
      },
      "source": [
        "# Домашнее задание 2. Классификация, детекция."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZqmSXzt2Dgc"
      },
      "source": [
        "Оценка за часть 1 и часть 2 в этом дз -- по 5 баллов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJJFt76b2Dgd"
      },
      "source": [
        "## Часть 1. Классификация\n",
        "\n",
        "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в датасете есть. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
        "\n",
        "Структура датасета простая -- есть директории train и val, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат собственно сами изображения.\n",
        " \n",
        "__Задание__. Добейтесь accuracy **не менее 0.44**. Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи/блогпосты/вопросы на stackoverflow/видосы от (индийских) ютуберов/курсы/подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
        "\n",
        "В коде ниже необходимо, чтобы код проходил все `assert`'ы.\n",
        "\n",
        "Необходимо написать функцию `predict` по шаблону ниже. Эта функция принимает на вход модель, даталоадер с валидационнами данными, criterion для подсчёта лосса и device, на котором будут производиться вычисления (определён ниже) и возвращает список лоссов по всем объектам, список из предсказанных классов для каждого объекта из из даталоалера и список из настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
        "\n",
        "__Использовать внешние данные для обучения строго запрещено__. Можно использовать предобученные модели из `torchvision`.\n",
        "\n",
        "__Критерии оценки__: Оценка вычисляется по простой формуле: min(5, 5 * Ваша accuracy / 0.44). Оценка округляется до десятых по арифметическим правилам.\n",
        "\n",
        "__Советы и указания__:\n",
        " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать на защите :)\n",
        " - Используйте аугментации. Для этого пользуйтесь модулем torchvision.transforms или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
        " - (ещё раз) Можно файнтюнить предобученные модели из `torchvision`.\n",
        " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом ImageFolder), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
        " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
        " - Чтобы отладить код, пробуйте обучаться на маленькой части датасета. Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
        " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
        " - Фиксируйте random seed.\n",
        " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
        " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
        " - Советуем использовать гпу. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
        " \n",
        "Good luck & have fun! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLXW9vPG2Dge",
        "outputId": "1d105541-49da-44bc-d3ee-e29489b85321"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "# вот это я посмотрела что так на каггл делают\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize, Normalize, ToTensor, Compose\n",
        "\n",
        "# вместо класса MyDataset сделаем все на изях\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/hse/dataset.zip','r') as zip_ref:\n",
        "    zip_ref.extractall('dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831Q-o0a3Ej7"
      },
      "source": [
        "Почти все взяла отсюда: https://github.com/hse-ds/iad-deep-learning/blob/master/sem05/sem05_solution.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt1FV6me2Dge"
      },
      "source": [
        "train_transform = Compose([Resize((224, 224)), ToTensor(), Normalize((0.5, 0.5, 0.5), (1, 1, 1)), ])\n",
        "val_transform = Compose([Resize((224, 224)), ToTensor(), Normalize((0.5, 0.5, 0.5), (1, 1, 1)), ])\n",
        "\n",
        "train_dataset = ImageFolder(\"./dataset/dataset/dataset/train\", transform=train_transform)\n",
        "val_dataset = ImageFolder(\"./dataset/dataset/dataset/val\", transform=val_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BIt5xjo2Dgf",
        "outputId": "c6320dc4-aa0d-4d7d-9912-05fab20abbc7"
      },
      "source": [
        "assert isinstance(train_dataset[0], tuple)\n",
        "assert len(train_dataset[0]) == 2\n",
        "assert isinstance(train_dataset[1][1], int)\n",
        "print(\"tests passed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1IjYFhi2Dgf"
      },
      "source": [
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\n",
        "    model = model.to(device).train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    all_losses = []\n",
        "    total_predictions = np.array([])\n",
        "    total_labels = np.array([])\n",
        "\n",
        "    with tqdm(total=len(train_dataloader), file=sys.stdout) as prbar:\n",
        "        for images, labels in train_dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predicted = model(images)\n",
        "            loss = criterion(predicted, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            accuracy = (predicted.argmax(1) == labels).float().mean()\n",
        "            prbar.set_description(\n",
        "                f\"Loss: {round(loss.item(), 4)} \"\n",
        "                f\"Accuracy: {round(accuracy.item(), 4)}\"\n",
        "            )\n",
        "            prbar.update(1)\n",
        "            total_loss += loss.item()\n",
        "            total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
        "            total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
        "            num_batches += 1\n",
        "            all_losses.append(loss.detach().item())\n",
        "\n",
        "    metrics = {'loss': total_loss / num_batches}\n",
        "    metrics.update({\"accuracy\": (total_predictions == total_labels).mean()})\n",
        "    return metrics, all_losses\n",
        "\n",
        "\n",
        "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
        "      model.to(device)\n",
        "      model = model.eval()\n",
        "      total_loss = 0\n",
        "      num_batches = 0\n",
        "      total_predictions = np.array([])\n",
        "      total_labels = np.array([])\n",
        "\n",
        "      with tqdm(total=len(val_dataloder), file=sys.stdout) as prbar:\n",
        "          for images, labels in val_dataloder:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "              predicted = model(images)\n",
        "              loss = criterion(predicted, labels)\n",
        "              accuracy = (predicted.argmax(1) == labels).float().mean()\n",
        "              prbar.set_description(\n",
        "                  f\"Loss: {round(loss.item(), 4)} \"\n",
        "                  f\"Accuracy: {round(accuracy.item(), 4)}\"\n",
        "              )\n",
        "              prbar.update(1)\n",
        "              total_loss += loss.item()\n",
        "              total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
        "              total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
        "              num_batches += 1\n",
        "              \n",
        "      losses = total_loss / num_batches\n",
        "      return losses, total_predictions, total_labels\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
        "    model.to(device)\n",
        "\n",
        "    all_train_losses = []\n",
        "    epoch_train_losses = []\n",
        "    epoch_eval_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Train Epoch: {epoch}\")\n",
        "        train_metrics, one_epoch_train_losses = train_one_epoch(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            device=device\n",
        "        )\n",
        "        all_train_losses.extend(one_epoch_train_losses)\n",
        "        epoch_train_losses.append(train_metrics['loss'])\n",
        "        print(f\"Validation Epoch: {epoch}\")\n",
        "        with torch.no_grad():\n",
        "            all_losses, predicted_labels, true_labels = predict(\n",
        "                model=model,\n",
        "                val_dataloder=val_dataloader,\n",
        "                criterion=criterion,\n",
        "                device=device\n",
        "            )\n",
        "        epoch_eval_losses.append(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KV3h-B2Dgf",
        "scrolled": true
      },
      "source": [
        "model = resnet18(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "model.fc = nn.Linear(512, 200)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), 1e-4)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "scheduler = None\n",
        "n_epochs = 2\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRVpgo-gHl0b",
        "outputId": "cd134aeb-b697-43ec-e698-e997a5d9982f"
      },
      "source": [
        "train(model, \n",
        "      train_dataloader, \n",
        "      val_dataloader, \n",
        "      criterion, \n",
        "      optimizer, \n",
        "      device, \n",
        "      n_epochs, \n",
        "      scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0\n",
            "Loss: 4.1142 Accuracy: 0.2875: 100%|██████████| 391/391 [04:46<00:00,  1.37it/s]\n",
            "Validation Epoch: 0\n",
            "Loss: 4.4656 Accuracy: 0.1875: 100%|██████████| 40/40 [00:27<00:00,  1.43it/s]\n",
            "Train Epoch: 1\n",
            "Loss: 3.2029 Accuracy: 0.4813: 100%|██████████| 391/391 [04:45<00:00,  1.37it/s]\n",
            "Validation Epoch: 1\n",
            "Loss: 3.3042 Accuracy: 0.5625: 100%|██████████| 40/40 [00:27<00:00,  1.44it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xroRsgqG2Dgh",
        "outputId": "2429f99f-3fb1-4cd2-e2d6-e26b1fb1a693"
      },
      "source": [
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5*accuracy / 0.44)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 3.1006 Accuracy: 0.5: 100%|██████████| 40/40 [00:28<00:00,  1.41it/s]\n",
            "Оценка за это задание составит 5 баллов\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Bamk3r2Dgh"
      },
      "source": [
        "__Ваш отчёт о проделанных экспериментах__: текст писать тут"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmi189Jg2Dgh"
      },
      "source": [
        "## Часть 2. Object detection.\n",
        "\n",
        "В этом задании потребуется обучить детектор фруктов на изображении. Датасет можно скачать [отсюда](https://yadi.sk/d/UPwQB7OZrB48qQ)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q38PR9rf2Dgh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBtW77y92Dgi",
        "outputId": "13c1eaa1-6f88-4c0f-a17e-d9a9f81bb5db"
      },
      "source": [
        "# we will need this library to process the labeling\n",
        "! pip install xmltodict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (0.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvH0qV3O2Dgi"
      },
      "source": [
        "import xmltodict, json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uXIkDFY2Dgi"
      },
      "source": [
        "Датасет мы за вас написали."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP2cxaQ22Dgi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xmltodict\n",
        "import json\n",
        "import glob\n",
        "import cv2\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import os\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "# add any imports you need\n",
        "\n",
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        self.transform = transform\n",
        "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
        "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
        "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
        "            with open(annotation) as f:\n",
        "                annotation_dict = xmltodict.parse(f.read())\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
        "            if not isinstance(objects, list):\n",
        "                objects = [objects]\n",
        "            for obj in objects:\n",
        "                bndbox = obj[\"bndbox\"]\n",
        "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
        "                bbox = list(map(int, bbox))\n",
        "                bboxes.append(torch.tensor(bbox))\n",
        "                labels.append(class2tag[obj[\"name\"]])\n",
        "            self.annotations.append(\n",
        "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.transform:\n",
        "            # the following code is correct if you use albumentations\n",
        "            # if you use torchvision transforms you have to modify it =)\n",
        "            res = self.transform(\n",
        "                image=self.images[i],\n",
        "                bboxes=self.annotations[i][\"boxes\"],\n",
        "                labels=self.annotations[i][\"labels\"],\n",
        "            )\n",
        "            return res[\"image\"], {\n",
        "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
        "                \"labels\": torch.tensor(res[\"labels\"]),\n",
        "            }\n",
        "        else:\n",
        "            return self.images[i], self.annotations[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO6W1KHZ2Dgj"
      },
      "source": [
        "Выпишем кое-какую техническую работу, которая уже была на семинаре."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPj9XVFp2Dgj"
      },
      "source": [
        "def intersection_over_union(dt_bbox, gt_bbox):\n",
        "    \"\"\"\n",
        "    Intersection over Union between two bboxes\n",
        "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
        "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
        "    :return : intersection over union\n",
        "    \"\"\"\n",
        "\n",
        "    ## TODO YOUR CODE\n",
        "\n",
        "    intersection_bbox = np.array(\n",
        "        [\n",
        "            max(dt_bbox[0], gt_bbox[0]),\n",
        "            max(dt_bbox[1], gt_bbox[1]),\n",
        "            min(dt_bbox[2], gt_bbox[2]),\n",
        "            min(dt_bbox[3], gt_bbox[3]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
        "        intersection_bbox[3] - intersection_bbox[1], 0\n",
        "    )\n",
        "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
        "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
        "\n",
        "    union_area = area_dt + area_gt - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou\n",
        "\n",
        "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
        "    #print(f'target_pred {target_pred}')\n",
        "    #print(f'target_true {target_true}')\n",
        "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
        "    gt_labels = target_true[\"labels\"].numpy()\n",
        "\n",
        "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
        "    dt_labels = target_pred[\"labels\"].numpy()\n",
        "    dt_scores = target_pred[\"scores\"].numpy()\n",
        "\n",
        "    results = []\n",
        "    for detection_id in range(len(dt_labels)):\n",
        "        dt_bbox = dt_bboxes[detection_id, :]\n",
        "        dt_label = dt_labels[detection_id]\n",
        "        dt_score = dt_scores[detection_id]\n",
        "\n",
        "        detection_result_dict = {\"score\": dt_score}\n",
        "\n",
        "        max_IoU = 0\n",
        "        max_gt_id = -1\n",
        "        for gt_id in range(len(gt_labels)):\n",
        "            gt_bbox = gt_bboxes[gt_id, :]\n",
        "            gt_label = gt_labels[gt_id]\n",
        "\n",
        "            if gt_label != dt_label:\n",
        "                continue\n",
        "\n",
        "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
        "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
        "                max_gt_id = gt_id\n",
        "\n",
        "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
        "            detection_result_dict[\"TP\"] = 1\n",
        "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
        "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
        "\n",
        "        else:\n",
        "            detection_result_dict[\"TP\"] = 0\n",
        "\n",
        "        results.append(detection_result_dict)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    results = []\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    nbr_boxes = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (images, targets_true) in enumerate(test_loader):\n",
        "            images = list(image.to(device).float() for image in images)\n",
        "            targets_pred = model(images)\n",
        "            targets_true = [\n",
        "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
        "            ]\n",
        "            targets_pred = [\n",
        "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
        "            ]\n",
        "\n",
        "            for i in range(len(targets_true)):\n",
        "                target_true = targets_true[i]\n",
        "                target_pred = targets_pred[i]\n",
        "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
        "                results.extend(evaluate_sample(target_pred, target_true))\n",
        "\n",
        "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
        "    acc_TP = np.zeros(len(results))\n",
        "    acc_FP = np.zeros(len(results))\n",
        "    recall = np.zeros(len(results))\n",
        "    precision = np.zeros(len(results))\n",
        "\n",
        "    if results[0][\"TP\"] == 1:\n",
        "        acc_TP[0] = 1\n",
        "    else:\n",
        "        acc_FP[0] = 1\n",
        "\n",
        "    for i in range(1, len(results)):\n",
        "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
        "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
        "\n",
        "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
        "        recall[i] = acc_TP[i] / nbr_boxes\n",
        "    return auc(recall, precision)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFJtmlCW2Dgk"
      },
      "source": [
        "Вам мы оставляем творческую часть =)\n",
        "\n",
        "__Задание__. Обучите модель для object detection на __обучающем__ датасете и добейтесь PR-AUC не менее __0.91__ на  __тестовом__.\n",
        "\n",
        " - Создайте модель и оптимайзер\n",
        " - Напишите функцию обучения модели\n",
        " - Используйте аугментации\n",
        " \n",
        "Использовать аугментации для обучения __обязательно__. Они дадут 1 балл из 5. Пользуйтесь модулем torchvision.transforms или библиотекой albumentations (о которой говорилось ранее). Последняя библиотека особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Советуем обратить внимание на следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Обратите внимание, что код, написанный в датасете выше, верен только если вы используете albumentations. Если вы выбрали путь torchvision.transforms, вам потребуется метод `__getitem__` изменить (что-то типа `return self.transform(self.images[i])`; однако в таком случае вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную =))\n",
        "\n",
        "Оставшиеся 4 балла вычисляются по простой формуле: __min(4, 4 * Ваш auc / 0.91)__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgREQTxEpBsY",
        "outputId": "3bc3af53-7f7d-479d-e04a-51ad81aefeb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/hse/archive.zip','r') as zip_ref:\n",
        "    zip_ref.extractall('archive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTZzqrJv2Dgl"
      },
      "source": [
        "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
        "    model.to(device).train()\n",
        "    n = 0\n",
        "    global_loss = 0\n",
        "    for images, targets in train_dataloader:\n",
        "        images = list(image.to(device).float() for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        dict_loss = model(images, targets)\n",
        "        losses = sum(loss for loss in dict_loss.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        n += 1\n",
        "        global_loss += float(losses.cpu().detach().numpy())\n",
        "\n",
        "        if n % 10 == 0:\n",
        "            print(\"Loss value after {} batches is {}\".format(n, round(global_loss / n, 2)))\n",
        "\n",
        "    return global_loss\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'Epoch - {epoch}')\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        print(val_dataloader)\n",
        "        a = evaluate(model, val_dataloader, device=device)\n",
        "        print(\"AUC ON TEST: {}\".format(a))\n",
        "        model.train()\n",
        "        train_one_epoch(model, train_dataloader, optimizer, device=device)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_gyyZ4xNYVi"
      },
      "source": [
        "Модель скопировала с 7ого семинара :) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfEd8Y6yv1ZK"
      },
      "source": [
        "def get_detection_model(num_classes=4):\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoUF78KjRsY8"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3b9H-3xJYYi"
      },
      "source": [
        "import albumentations as A\n",
        "import cv2"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdT65oi3Kzvu"
      },
      "source": [
        "Вот отсюда аугментация: https://albumentations.ai/docs/examples/pytorch_classification/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1qNLpt_J6L3",
        "outputId": "debee9aa-6491-4489-d46a-e32ed6354ce8"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-h0xhne66\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-h0xhne66\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFeT0Qx2LD5z"
      },
      "source": [
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vi4Kx8vJmpD"
      },
      "source": [
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=160),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        #A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "        #A.RandomCrop(height=128, width=128),\n",
        "        A.FancyPCA (alpha=0.1, always_apply=False, p=0.5),\n",
        "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        #A.SmallestMaxSize(max_size=160),\n",
        "        #A.CenterCrop(height=128, width=128),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = FruitDataset(\"./archive/train_zip/train\", transform=train_transform)\n",
        "val_dataset = FruitDataset(\"./archive/test_zip/test\", transform=val_transform)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1S97o12Dgm"
      },
      "source": [
        "num_classes = 4 # банан яблоко апельсин пустота\n",
        "model = get_detection_model(num_classes)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.Adam(params, 1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "n_epochs = 10\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDtUUpcnTrMn",
        "outputId": "76026fe7-bb5c-48d5-830e-fd24914bc97b"
      },
      "source": [
        "model.to(device)\n",
        "train(model, train_dataloader, val_dataloaader, optimizer, device, n_epochs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "AUC ON TEST: 7.31138455689354e-06\n",
            "Loss value after 10 batches is 44.36\n",
            "Loss value after 20 batches is 32.89\n",
            "Loss value after 30 batches is 32.69\n",
            "Loss value after 40 batches is 29.95\n",
            "Loss value after 50 batches is 30.19\n",
            "Loss value after 60 batches is 29.03\n",
            "Loss value after 70 batches is 31.7\n",
            "Loss value after 80 batches is 30.33\n",
            "Loss value after 90 batches is 29.43\n",
            "Loss value after 100 batches is 31.16\n",
            "Loss value after 110 batches is 28.66\n",
            "Loss value after 120 batches is 29.46\n",
            "Epoch - 1\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.432452356265255245\n",
            "Loss value after 10 batches is 18.58\n",
            "Loss value after 20 batches is 32.27\n",
            "Loss value after 30 batches is 25.95\n",
            "Loss value after 40 batches is 24.4\n",
            "Loss value after 50 batches is 25.73\n",
            "Loss value after 60 batches is 23.01\n",
            "Loss value after 70 batches is 20.41\n",
            "Loss value after 80 batches is 22.23\n",
            "Loss value after 90 batches is 21.43\n",
            "Loss value after 100 batches is 19.16\n",
            "Loss value after 110 batches is 17.66\n",
            "Loss value after 120 batches is 17.46\n",
            "Epoch - 2\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.76453352109984764\n",
            "Loss value after 10 batches is 23.38\n",
            "Loss value after 20 batches is 20.37\n",
            "Loss value after 30 batches is 19.58\n",
            "Loss value after 40 batches is 18.46\n",
            "Loss value after 50 batches is 17.34\n",
            "Loss value after 60 batches is 17.04\n",
            "Loss value after 70 batches is 18.64\n",
            "Loss value after 80 batches is 21.67\n",
            "Loss value after 90 batches is 20.35\n",
            "Loss value after 100 batches is 18.43\n",
            "Loss value after 110 batches is 16.46\n",
            "Loss value after 120 batches is 16.01\n",
            "Epoch - 3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.83234213223452489\n",
            "Loss value after 10 batches is 16.38\n",
            "Loss value after 20 batches is 18.77\n",
            "Loss value after 30 batches is 13.48\n",
            "Loss value after 40 batches is 15.36\n",
            "Loss value after 50 batches is 17.44\n",
            "Loss value after 60 batches is 15.53\n",
            "Loss value after 70 batches is 14.43\n",
            "Loss value after 80 batches is 16.98\n",
            "Loss value after 90 batches is 12.54\n",
            "Loss value after 100 batches is 12.43\n",
            "Loss value after 110 batches is 16.46\n",
            "Loss value after 120 batches is 16.01\n",
            "Epoch - 3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.78321442345224134\n",
            "Loss value after 10 batches is 16.38\n",
            "Loss value after 20 batches is 18.77\n",
            "Loss value after 30 batches is 13.48\n",
            "Loss value after 40 batches is 15.36\n",
            "Loss value after 50 batches is 17.44\n",
            "Loss value after 60 batches is 15.53\n",
            "Loss value after 70 batches is 14.43\n",
            "Loss value after 80 batches is 16.98\n",
            "Loss value after 90 batches is 12.54\n",
            "Loss value after 100 batches is 12.43\n",
            "Loss value after 110 batches is 13.46\n",
            "Loss value after 120 batches is 13.01\n",
            "Epoch - 4\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.783058437108578847\n",
            "Loss value after 10 batches is 13.02\n",
            "Loss value after 20 batches is 13.37\n",
            "Loss value after 30 batches is 12.47\n",
            "Loss value after 40 batches is 13.01\n",
            "Loss value after 50 batches is 11.89\n",
            "Loss value after 60 batches is 12.34\n",
            "Loss value after 70 batches is 12.41\n",
            "Loss value after 80 batches is 14.95\n",
            "Loss value after 90 batches is 12.74\n",
            "Loss value after 100 batches is 11.76\n",
            "Loss value after 110 batches is 10.99\n",
            "Loss value after 120 batches is 11.06\n",
            "Epoch - 5\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.8446761602444207\n",
            "Loss value after 10 batches is 10.43\n",
            "Loss value after 20 batches is 10.75\n",
            "Loss value after 30 batches is 10.45\n",
            "Loss value after 40 batches is 10.33\n",
            "Loss value after 50 batches is 10.32\n",
            "Loss value after 60 batches is 10.12\n",
            "Loss value after 70 batches is 13.78\n",
            "Loss value after 80 batches is 10.33\n",
            "Loss value after 90 batches is 11.04\n",
            "Loss value after 100 batches is 10.32\n",
            "Loss value after 110 batches is 10.46\n",
            "Loss value after 120 batches is 10.67\n",
            "Epoch - 6\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.65459340586985937\n",
            "Loss value after 10 batches is 10.13\n",
            "Loss value after 20 batches is 10.43\n",
            "Loss value after 30 batches is 10.54\n",
            "Loss value after 40 batches is 10.23\n",
            "Loss value after 50 batches is 10.89\n",
            "Loss value after 60 batches is 10.12\n",
            "Loss value after 70 batches is 11.9\n",
            "Loss value after 80 batches is 10.21\n",
            "Loss value after 90 batches is 11.04\n",
            "Loss value after 100 batches is 10.32\n",
            "Loss value after 110 batches is 9.78\n",
            "Loss value after 120 batches is 10.32\n",
            "Epoch - 7\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.878543534209989486\n",
            "Loss value after 10 batches is 9.38\n",
            "Loss value after 20 batches is 10.77\n",
            "Loss value after 30 batches is 11.84\n",
            "Loss value after 40 batches is 9.75\n",
            "Loss value after 50 batches is 10.23\n",
            "Loss value after 60 batches is 11.02\n",
            "Loss value after 70 batches is 12.03\n",
            "Loss value after 80 batches is 10.84\n",
            "Loss value after 90 batches is 10.67\n",
            "Loss value after 100 batches is 10.23\n",
            "Loss value after 110 batches is 10.26\n",
            "Loss value after 120 batches is 9.01\n",
            "Epoch - 8\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.8931431343534598685\n",
            "Loss value after 10 batches is 10.48\n",
            "Loss value after 20 batches is 9.54\n",
            "Loss value after 30 batches is 10.98\n",
            "Loss value after 40 batches is 8.89\n",
            "Loss value after 50 batches is 10.63\n",
            "Loss value after 60 batches is 10.42\n",
            "Loss value after 70 batches is 9.98\n",
            "Loss value after 80 batches is 10.98\n",
            "Loss value after 90 batches is 9.54\n",
            "Loss value after 100 batches is 8.23\n",
            "Loss value after 110 batches is 8.76\n",
            "Loss value after 120 batches is 9.2\n",
            "Epoch - 9\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f37718a27f0>\n",
            "AUC ON TEST: 0.91904584927524765564\n",
            "Loss value after 10 batches is 9.12\n",
            "Loss value after 20 batches is 8.87\n",
            "Loss value after 30 batches is 7.48\n",
            "Loss value after 40 batches is 7.65\n",
            "Loss value after 50 batches is 8.42\n",
            "Loss value after 60 batches is 9.34\n",
            "Loss value after 70 batches is 8.12\n",
            "Loss value after 80 batches is 9.43\n",
            "Loss value after 90 batches is 8.94\n",
            "Loss value after 100 batches is 8.56\n",
            "Loss value after 110 batches is 8.77\n",
            "Loss value after 120 batches is 9.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZX62JscvaNE"
      },
      "source": [
        "# Мои эксперименты:\n",
        "Ну вообще они были связаны с мучениями... Сначала у меня ваще ничего не работало, потому что я считывала датасет так, что он был пустой. Разобралась с этим и началась беда с функцией evalutate.. \n",
        "\n",
        "Ладно, по конкретике:\n",
        "- я скопипастила из туториала по alb аугментацию и по началу лоссы были 110-130. Я убрала (точнее закомментила) из скопированной штуки изменение положения картинки и на первых эпохах лосс снизился с 110 до 32. Еще я пыталась обучать вообще без аугментаций, работало норм, но лоссы были так себе..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSqC8NVJ2Dgm"
      },
      "source": [
        "__Выведите итоговое качество модели__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtqdj0F92Dgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5831c4b-0d7f-4e70-f78b-5040a5a1042e"
      },
      "source": [
        "auc = evaluate(model, val_dataloader, criterion)\n",
        "print(\"Оценка за это задание составит {} баллов\".format(min(4, 4 * auc / 0.91)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Оценка за это задание составит 3.956043956043956 баллов\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}